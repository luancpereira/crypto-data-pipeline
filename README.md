
# Coleta e Armazenamento de Dados de Criptomoedas

---

## Tecnologias Utilizadas

- **Python**: Utilizado para chamadas √† API, transforma√ß√£o e orquestra√ß√£o dos dados.
- **Google Cloud Functions + Cloud Run**: Execu√ß√£o e orquestra√ß√£o do pipeline. O **Cloud Run** executa a chamada da API e realiza o salvamento dos dados no BigQuery de forma perform√°tica.
- **BigQuery (Google Cloud Platform)**: Armazenamento estruturado e escal√°vel para os dados coletados.
- **Apache Airflow**: Orquestra√ß√£o e agendamento do pipeline de dados.
- **Looker**: Visualiza√ß√£o dos dados tratados.

---

## Estrutura do Projeto no Cloud Run

O diret√≥rio `cloud-run-coincap-api` cont√©m toda a l√≥gica respons√°vel pela coleta, transforma√ß√£o e carga de dados.

<details>
  <summary><strong>üîß bigquery_functions.py</strong></summary>

Fun√ß√µes auxiliares para intera√ß√£o com o BigQuery:

- `insert_into_bigquery(rows, table, chunk_size)`:  
  Insere dados em qualquer tabela do BigQuery de forma escal√°vel via chunking.

- `check_execution_date(crypto)`:  
  Verifica a √∫ltima execu√ß√£o de uma determinada criptomoeda para evitar reprocessamento.

- `insert_log_entry(error)`:  
  Insere registros de erro na tabela de log.

</details>

<details>
  <summary><strong>etl.py</strong></summary>

Respons√°vel pela transforma√ß√£o dos dados para os formatos compat√≠veis com o BigQuery:

- `transform_assets_data(data)`  
- `transform_rates_data(data)`  
- `transform_assets_history_data(data)`  

Essas fun√ß√µes garantem que os dados estejam no formato correto e com os tipos apropriados para evitar falhas de carregamento.

</details>

<details>
  <summary><strong>externals_apis.py</strong></summary>

Respons√°vel pelas chamadas externas √† API CoinCap:

- `get_assets_data()`  
- `get_rates_data()`  
- `get_assets_history_data()`  

Cada fun√ß√£o realiza tratamento de par√¢metros, erros e retorno das respostas de forma padronizada.

</details>

<details>
  <summary><strong>service.py</strong></summary>

Coordena o fluxo completo de dados:

- `process_assets_data()`  
- `process_rates_data()`  
- `process_assets_history_data()`:  
  Executa uma verifica√ß√£o no BigQuery para garantir que o hist√≥rico da cripto n√£o foi processado no dia antes de continuar. Processa dados individualmente por cripto, em uma estrutura de repeti√ß√£o.

</details>

<details>
  <summary><strong>utils.py</strong></summary>

Fun√ß√µes utilit√°rias reutilizadas em diversas partes do projeto. Auxiliam na padroniza√ß√£o e organiza√ß√£o da l√≥gica.

</details>

<details>
  <summary><strong>main.py</strong></summary>

Ponto de entrada principal da aplica√ß√£o no Cloud Run:

- Realiza a chamada das fun√ß√µes de `service.py`
- Trata erros e logs
- Realiza leitura do token e do corpo da requisi√ß√£o enviada ao endpoint

</details>

<details>
  <summary><strong>constants.py</strong></summary>

Armazena vari√°veis globais, configura√ß√µes e nomes de tabelas:

```python
import os

class Config:
    API_URL = os.environ.get("API_URL")
    BIGQUERY_DATASET = "cadastra-teste.APIcripto"
    BATCH_SIZE = 500

class TableNames:
    ASSETS = f"{Config.BIGQUERY_DATASET}.assets"
    RATES = f"{Config.BIGQUERY_DATASET}.rates"
    ASSETS_HISTORY = f"{Config.BIGQUERY_DATASET}.assets_history"
    LOG_EXECUTION = f"{Config.BIGQUERY_DATASET}.log_execution"
```

</details>

---

## Funcionamento Geral no Cloud Run

A aplica√ß√£o foi constru√≠da para ser executada via **Google Cloud Run**, onde o endpoint exposto √© respons√°vel por:

1. Realizar chamadas √†s APIs de criptomoedas;
2. Transformar os dados para o formato do BigQuery;
3. Verificar execu√ß√µes anteriores (quando necess√°rio);
4. Inserir os dados em lote no BigQuery de forma escal√°vel (`chunking`);
5. Registrar falhas ou erros no log.

---