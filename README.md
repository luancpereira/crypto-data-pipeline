
# Coleta e Armazenamento de Dados de Criptomoedas

**Link do Looker**: https://lookerstudio.google.com/s/vgb9fT77bPs

## Dashboard de Criptomoedas

Este projeto tem como resultado final um dashboard interativo desenvolvido no Looker Studio, com foco no monitoramento de criptomoedas. O painel exibe:

- Cota√ß√£o atual em USD e BRL
- Varia√ß√£o percentual nas √∫ltimas 24 horas
- Criptomoeda com maior valoriza√ß√£o no dia
- Volume total negociado nas √∫ltimas 24h
- Market Cap agregado
- Distribui√ß√£o de volume por criptomoeda em gr√°fico de pizza
- An√°lise hor√°ria da volatilidade dos pre√ßos

Com isso, √© poss√≠vel obter uma vis√£o clara e r√°pida do mercado cripto, facilitando decis√µes estrat√©gicas e o acompanhamento de tend√™ncias.

---

# Configura√ß√£o e Execu√ß√£o do Projeto

## Pr√©-requisitos

### 1. Configura√ß√µes no Google Cloud Platform

Para executar o projeto em nuvem, √© necess√°rio ativar os seguintes servi√ßos do **Google Cloud Platform**:

- **Cloud Run** - Para hospedagem do servi√ßo de coleta
- **BigQuery** - Para armazenamento e processamento dos dados

### 2. Service Account

Gere o arquivo JSON de **Service Account** no GCP com as permiss√µes necess√°rias para:
- Acessar o BigQuery
- Executar servi√ßos no Cloud Run

## Configura√ß√£o do Apache Airflow

### 1. Instala√ß√£o do Docker

Certifique-se de ter o **Docker** e **Docker Compose** instalados em sua m√°quina.

### 2. Estrutura de Diret√≥rios

Crie a seguinte estrutura de diret√≥rios no seu ambiente Airflow:

```
airflow/
‚îú‚îÄ‚îÄ dags/
‚îú‚îÄ‚îÄ credenciais/
‚îÇ   ‚îî‚îÄ‚îÄ gcp-sa.json
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ .env
```

### 3. Configura√ß√£o do Arquivo .env

Crie um arquivo `.env` na raiz do diret√≥rio do Airflow com as seguintes vari√°veis:

```bash
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=/home/luan/airflow  # Substitua pelo seu diret√≥rio local do Airflow
```

> **Nota**: Ajuste o `AIRFLOW_PROJ_DIR` para o caminho correto do seu diret√≥rio do Airflow.

### 4. Configura√ß√£o do Docker Compose

No arquivo `docker-compose.yaml`, adicione o seguinte mapeamento de volume na se√ß√£o `volumes`:

```yaml
volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/credenciais/gcp-sa.json:/opt/airflow/gcp-sa.json
```

Esta configura√ß√£o mapeia suas credenciais do GCP para dentro do container do Airflow.

### 5. Adicionando a DAG

Copie o arquivo `execute_all_services.py` para a pasta `dags/` criada anteriormente.

## Executando o Projeto

### 1. Inicializa√ß√£o do Airflow

Execute o comando para inicializar o Airflow:

```bash
sudo docker compose up
```

Aguarde o carregamento completo do sistema.

### 2. Acesso √† Interface Web

Ap√≥s a inicializa√ß√£o, acesse a interface web do Airflow em:

```
http://localhost:8080/
```

### 3. Credenciais de Acesso

Use as seguintes credenciais padr√£o para fazer login:

- **Usu√°rio**: `airflow`
- **Senha**: `airflow`

### 4. Verifica√ß√£o da DAG

Na interface web, voc√™ poder√° visualizar e monitorar a DAG `execute_all_services` e suas execu√ß√µes hor√°rias.

---

## Tecnologias Utilizadas

- **Python**: Utilizado para chamadas √† API, transforma√ß√£o e orquestra√ß√£o dos dados.
- **Google Cloud Functions + Cloud Run**: Execu√ß√£o e orquestra√ß√£o do pipeline. O **Cloud Run** executa a chamada da API e realiza o salvamento dos dados no BigQuery de forma perform√°tica.
- **BigQuery (Google Cloud Platform)**: Armazenamento estruturado e escal√°vel para os dados coletados.
- **Apache Airflow**: Orquestra√ß√£o e agendamento do pipeline de dados.
- **Looker**: Visualiza√ß√£o dos dados tratados.

---

## Estrutura do Projeto no Cloud Run

O diret√≥rio `cloud-run-coincap-api` cont√©m toda a l√≥gica respons√°vel pela coleta, transforma√ß√£o e carga de dados.

<details>
  <summary><strong>bigquery_functions.py</strong></summary>

Fun√ß√µes auxiliares para intera√ß√£o com o BigQuery:

- `insert_into_bigquery(rows, table, chunk_size)`:  
  Insere dados em qualquer tabela do BigQuery de forma escal√°vel via chunking.

- `check_execution_date(crypto_id, current_date)`:  
  Verifica a √∫ltima execu√ß√£o de uma determinada criptomoeda para evitar reprocessamento.

- `insert_log_entry(crypto_id, status, json_error, timestamp_hour)`:  
  Insere registros de erro na tabela de log.
</details>

<details>
  <summary><strong>etl.py</strong></summary>

Respons√°vel pela transforma√ß√£o dos dados para os formatos compat√≠veis com o BigQuery:

- `transform_assets_data(json_data)`  
- `transform_rates_data(json_data)`  
- `transform_assets_history_data(json_data, crypto_id, execution_date)`  
</details>

<details>
  <summary><strong>externals_apis.py</strong></summary>

Respons√°vel pelas chamadas externas √† API CoinCap:

- `get_assets_data(token, ids=None)`  
- `get_rates_data(token, ids=None)`  
- `get_assets_history_data(token, crypto_id, start_timestamp, end_timestamp)`  
</details>

<details>
  <summary><strong>service.py</strong></summary>

Coordena o fluxo completo de dados:

- `process_assets_data(token, cryptos)`  
- `process_rates_data(token, cryptos)`  
- `process_assets_history_data(token, cryptos)`  
</details>

<details>
  <summary><strong>utils.py</strong></summary>

Fun√ß√µes utilit√°rias reutilizadas em diversas partes do projeto.
</details>

<details>
  <summary><strong>main.py</strong></summary>

Ponto de entrada principal da aplica√ß√£o no Cloud Run.
</details>

<details>
  <summary><strong>constants.py</strong></summary>

Armazena vari√°veis globais, configura√ß√µes e nomes de tabelas:

```python
import os

class Config:
    API_URL = os.environ.get("API_URL")
    BIGQUERY_DATASET = "cadastra-teste.APIcripto"
    BATCH_SIZE = 500

class TableNames:
    ASSETS = f"{Config.BIGQUERY_DATASET}.assets"
    RATES = f"{Config.BIGQUERY_DATASET}.rates"
    ASSETS_HISTORY = f"{Config.BIGQUERY_DATASET}.assets_history"
    LOG_EXECUTION = f"{Config.BIGQUERY_DATASET}.log_execution"
```
</details>

---

## Funcionamento Geral no Cloud Run

A aplica√ß√£o √© executada via **Google Cloud Run**, com um endpoint exposto que:

1. Realiza chamadas √†s APIs de criptomoedas;
2. Transforma os dados para o formato do BigQuery;
3. Verifica execu√ß√µes anteriores (quando necess√°rio);
4. Insere os dados em lote no BigQuery de forma escal√°vel (chunking);
5. Registra falhas ou erros no log.

---

## Estrutura de Armazenamento no BigQuery

A arquitetura de dados segue o conceito de **camadas**, com dois conjuntos de dados principais:

### üîπ `APIcripto` (Camada Bruta)

Conjunto de dados que armazena informa√ß√µes brutas obtidas diretamente da API e tamb√©m procedures intermedi√°rias de transforma√ß√£o.

#### Tabelas brutas

- `cadastra-teste.APIcripto.assets`  
- `cadastra-teste.APIcripto.rates`  
- `cadastra-teste.APIcripto.assets_history`  

> As queries de defini√ß√£o est√£o na pasta: `bigquery/tables`

#### Procedures

> Localizadas em: `bigquery/procedures`

- `best_performers_last_24h`:  
  Gera ranking das criptomoedas com **melhor e pior performance nas √∫ltimas 24 horas**, categorizadas em:
  - `up`: crescimento
  - `down`: queda

- `crypto_analysis_by_hour`:  
  Executa an√°lise hor√°ria das criptos e evita reprocessamento de dados j√° existentes.

- `latest_rates`:  
  Atualiza as **√∫ltimas cota√ß√µes** dispon√≠veis das criptomoedas.

---

### `APIcripto_gold` (Camada Tratada)

Conjunto com os **dados prontos para an√°lise**, resultado das procedures da camada bruta.

#### Tabelas tratadas

- `cadastra-teste.APIcripto_gold.latest_rates`  
- `cadastra-teste.APIcripto_gold.crypto_analysis_by_hour`  
- `cadastra-teste.APIcripto_gold.best_performers_last_24h`  

Essas tabelas servem como base para visualiza√ß√µes no **Looker** e demais an√°lises de neg√≥cio.

---

# Orquestra√ß√£o com Apache Airflow

O processo de orquestra√ß√£o do pipeline de dados √© realizado por meio do **Apache Airflow**, utilizando uma DAG configurada para rodar de **hora em hora**.

## Estrutura da DAG

A DAG principal est√° localizada no diret√≥rio:
```
airflow-execute-services/execute_all_services.py
```

### Fluxo de Execu√ß√£o

A DAG realiza duas etapas principais executadas sequencialmente:

## 1. Execu√ß√£o do Servi√ßo no Cloud Run

A fun√ß√£o `execute_services()` √© respons√°vel por acionar o servi√ßo exposto no **Cloud Run** atrav√©s do endpoint:

```
https://coincap-api-753104042367.us-central1.run.app
```

### Par√¢metros Enviados

Na chamada do servi√ßo, s√£o enviadas as seguintes criptomoedas de interesse:

```python
cryptos = [
    "bitcoin",
    "ethereum", 
    "tether",
    "xrp",
    "binance-coin",
    "solana",
    "usd-coin",
    "tron",
    "dogecoin",
    "steth"
]
```

### Funcionalidades do Servi√ßo

Este servi√ßo executa toda a l√≥gica de:
- **Coleta** de dados das criptomoedas
- **Transforma√ß√£o** dos dados coletados
- **Carga** dos dados no BigQuery
- **Registro de erros** (quando necess√°rio)

## 2. Execu√ß√£o das Procedures no BigQuery

Somente ap√≥s a execu√ß√£o **bem-sucedida** do servi√ßo no Cloud Run, a DAG avan√ßa para a pr√≥xima etapa.

### Procedures Executadas

A execu√ß√£o das procedures √© realizada atrav√©s da fun√ß√£o `execute_bigquery_procedure(procedure_query)` e inclui:

- `best_performers_last_24h`
- `crypto_analysis_by_hour`
- `latest_rates`

### Processamento Concorrente

Como essas procedures s√£o **independentes** entre si, elas s√£o executadas de forma **concorrente**, otimizando o tempo de processamento da DAG.

## Frequ√™ncia de Execu√ß√£o

- **Intervalo**: A cada hora
- **Tipo**: Agendamento autom√°tico via Apache Airflow

## Depend√™ncias

1. **Apache Airflow** - Orquestrador principal
2. **Google Cloud Run** - Hospedagem do servi√ßo de coleta
3. **Google BigQuery** - Armazenamento e processamento de dados

---